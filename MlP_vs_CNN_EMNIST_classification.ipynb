{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_xTUT5UB9S_v",
        "outputId": "02be1f64-dca9-4e0e-99d1-305f578fe844"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip to /Users/rhyscooper/Downloads/EMNIST/EMNIST/raw/gzip.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 35%|███▍      | 194629376/561753746 [04:07<07:46, 786337.07it/s]  \n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "File not found or corrupted.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/jg/m6l2g86n6vl9c_nbqcftb2hh0000gn/T/ipykernel_13939/472626683.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#Loads the EMNIST training and test set. NOTE: If running on google collab, set root='./Downloads', if running on a local IDA , the code will likely have to be amended to specify a filepath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/Users/rhyscooper/Downloads/EMNIST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEMNIST\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/Users/rhyscooper/Downloads/EMNIST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'balanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, split, **kwargs)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_split_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mgzip_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gzip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mgzip_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgzip_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;31m# check integrity of downloaded file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File not found or corrupted.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: File not found or corrupted."
          ]
        }
      ],
      "source": [
        "#The librarys imported are numpy, time, matplotlib, seaborn, Pytorch, TorchVision, Math, Sci-kit Learn.\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.datasets import EMNIST\n",
        "import torchvision.transforms as transforms\n",
        "import math \n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "\n",
        "# Set the device to GPU if available, otherwise use CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Below is transformation to applied to the images. They are transformed with a normalisaiton that normalizes the pixel values to ensure mean 0 and std 1, to increase the speed of convergence. \n",
        "# the ToTensor converts PIL image format to a PyTorch tensor. \n",
        "transform = transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5,), (0.5,)) \n",
        "                ]) \n",
        "\n",
        "#Loads the EMNIST training and test set. NOTE: If running on google collab, set root='./Downloads', if running on a local IDA , the code will likely have to be amended to specify a filepath\n",
        "train_set = EMNIST(root='/Users/rhyscooper/Downloads/EMNIST', split ='balanced', train=True, download=True, transform=transform)\n",
        "test_set = EMNIST(root='/Users/rhyscooper/Downloads/EMNIST', split ='balanced', train=False, download=True, transform=transform)\n",
        "                                      \n",
        "class Model():\n",
        "    def __init__(self, model, train_set, test_set):\n",
        "        \n",
        "        #Initialise the model (MLP or CNN), the train data set and the testing data set\n",
        "        self.model = model\n",
        "        self.train_set = train_set\n",
        "        self.test_set = test_set\n",
        "\n",
        "        #Initialise random seeds for reproducibility\n",
        "        torch.manual_seed(0)\n",
        "        np.random.seed(0)\n",
        "\n",
        "        #Create a class_names dictionary that maps each class number (0-46) to the corresponding class name (0,...9, A,...Z, a,...t)\n",
        "        class_names = self.test_set.classes\n",
        "        self.labels_map = {}\n",
        "\n",
        "        for class_number, class_name in enumerate(class_names):\n",
        "            self.labels_map[class_number] = class_name\n",
        "          \n",
        "        #Initialise the train and test loaders based on the input train_set and test_set\n",
        "        self.train_loader = torch.utils.data.DataLoader(self.train_set, batch_size=64, shuffle=True)\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.test_set, batch_size=64, shuffle=False)\n",
        "   \n",
        "    def describe_dataset(self, data):\n",
        "        '''\n",
        "        For given input dataset (train_set or test_set), plot the first 6 samples along with their descriptive class labels,\n",
        "        and print out how many samples are in the dataset\n",
        "        '''\n",
        "\n",
        "        if \"Train\" in str(data):\n",
        "            data_set_name = \"train_data\"\n",
        "        elif \"Test\" in str(data):\n",
        "            data_set_name = \"test_data\"\n",
        "\n",
        "        #Plot out the 'header' of the dataset, corresponding to the first 6 images and labels\n",
        "        figure = plt.figure(figsize=(8, 7))\n",
        "        plt.title(f\"The First 6 Samples In {data_set_name}\\nNumber of samples: {len(data)}\")\n",
        "        plt.axis(\"off\")\n",
        "        rows, cols = 2, 3\n",
        "\n",
        "        for i in range(1, cols * rows + 1):\n",
        "            img, label = data[i-1]\n",
        "            img = img.mT\n",
        "            figure.add_subplot(rows, cols, i)\n",
        "            plt.title(self.labels_map[label])\n",
        "            plt.axis(\"off\")\n",
        "            plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "            \n",
        "        plt.show()\n",
        "\n",
        "    def train(self, n_epochs=10, verbose=True):\n",
        "      '''\n",
        "      Trains the model for n_epochs, iterating over self.train_loader and for each batch caclulating the loss and implementing backpropagation.\n",
        "      Verbose=True prints out the current epoch, current step (batch), and current loss.\n",
        "      '''\n",
        "\n",
        "      #Put model into training mode. Initialise start time, train loss, and train accuracy variables\n",
        "      self.model.train()\n",
        "      self.n_epochs = n_epochs\n",
        "      start_time = time.time()\n",
        "      total_step = len(self.train_loader)\n",
        "      train_loss = []\n",
        "      train_acc = []\n",
        "\n",
        "      #Iterate over n_epochs\n",
        "      for epoch in range(n_epochs):\n",
        "        epoch_loss = []\n",
        "        epoch_correct_predictions = 0 \n",
        "        total_predictions = 0\n",
        "  \n",
        "        #Iterate over the batches in train_loader\n",
        "        for i, (images, labels) in enumerate(self.train_loader):\n",
        "          if self.model.name == 'MLP':  \n",
        "            images, labels = images.reshape(-1, 28*28).to(device), labels.to(device)\n",
        "          outputs = self.model(images)\n",
        "          loss = self.model.loss(outputs, labels)\n",
        "          \n",
        "          #Implement L1 regularisation if required\n",
        "          if self.model.l1_lambda > 0:\n",
        "            l1_reg = torch.tensor(0., requires_grad=True)\n",
        "            for name, param in self.model.named_parameters():\n",
        "              if 'weight' in name:\n",
        "                l1_reg = l1_reg + torch.norm(param, 1)\n",
        "            loss += self.model.l1_lambda * l1_reg\n",
        "      \n",
        "          #Implement L2 regularisation if required\n",
        "          if self.model.l2_lambda > 0:\n",
        "            l2_reg = torch.tensor(0., requires_grad=True)\n",
        "            for name, param in self.model.named_parameters():\n",
        "              if 'weight' in name:\n",
        "                l2_reg = l2_reg + torch.norm(param, 2)\n",
        "            loss += self.model.l2_lambda * l2_reg\n",
        "\n",
        "          #Update model weights and hyperparameters\n",
        "          self.model.optimiser.zero_grad()\n",
        "          loss.backward()\n",
        "          self.model.optimiser.step()\n",
        "\n",
        "          #Update variables tracking loss, predictions, and truth labels \n",
        "          epoch_loss.append(loss.item())\n",
        "          predicted_labels = torch.max(outputs.data, 1)[1]\n",
        "          epoch_correct_predictions += predicted_labels.eq(labels).sum().item()  \n",
        "          total_predictions += labels.size(0)\n",
        "          \n",
        "          #Print out information about current epoch, step, and loss if verbose=True\n",
        "          if verbose==True:\n",
        "            if (i+1) % 100 == 0:\n",
        "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                    .format(epoch+1, n_epochs, i+1, total_step, loss.item()))\n",
        "\n",
        "        #Update learning rate according to learning rate scheduler hyperparameter         \n",
        "        self.model.scheduler.step()\n",
        "\n",
        "        #Calculate loss and accuracy\n",
        "        train_loss.append(np.mean(epoch_loss))\n",
        "        epoch_accuracy = epoch_correct_predictions / total_predictions\n",
        "        train_acc.append(epoch_accuracy)\n",
        "\n",
        "      self.train_acc, self.train_loss = train_acc, train_loss\n",
        "      self.train_time = time.time() - start_time\n",
        "\n",
        "      return self.train_acc, self.train_loss\n",
        "    \n",
        "    def plot(self):\n",
        "        '''\n",
        "        Display a twin-axes graph showing the training accuracy and loss over each epoch\n",
        "        '''\n",
        "        fig, ax1 = plt.subplots() \n",
        "        plt.title(f\"Training Accuracy and Loss for {self.model.name}\")\n",
        "\n",
        "        x = [str(i+1) for i in range(self.n_epochs)]\n",
        "        y1 = np.array(self.train_acc) * 100\n",
        "        y2 = np.array(self.train_loss)\n",
        "\n",
        "        ax1.set_xlabel('Epochs') \n",
        "        ax1.set_ylabel('Accuracy %', color='black') \n",
        "        ax1.set_ylim(bottom=min(y1)*0.95, top=max(min(y1) + 2, max(y1)*1.05))\n",
        "        acc_plot = ax1.plot(x, y1, color='red', label='Accuracy') \n",
        "\n",
        "        # Adding Twin Axes\n",
        "        ax2 = ax1.twinx()   \n",
        "        ax2.set_ylabel('Loss', color='black') \n",
        "        ax2.set_ylim(bottom=min(y2)*0.90, top=max(min(y2) + 1, max(y2)*1.05))\n",
        "        loss_plot = ax2.plot(x, y2, color='blue', label='Loss') \n",
        "\n",
        "        plots = acc_plot + loss_plot\n",
        "        labels = [l.get_label() for l in plots]\n",
        "        plt.legend(plots, labels, loc='lower center')\n",
        "\n",
        "        # Show plot\n",
        "        plt.show()\n",
        "\n",
        "    def test(self):\n",
        "        '''\n",
        "        Test the model using test_loader, tracking the predicted values and corresponding truth labels\n",
        "        '''\n",
        "\n",
        "        #Initialise variables to keep track of model predictions, truth labels, and test loss\n",
        "        self.predictions = []\n",
        "        self.truth_labels = []\n",
        "        self.test_loss = []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct_predictions = 0\n",
        "            total_predictions = 0  \n",
        "\n",
        "            #Iterate through images and labels in test_loader\n",
        "            for images, labels in self.test_loader:\n",
        "                if self.model.name == 'MLP':  \n",
        "                    images, labels = images.reshape(-1, 28*28).to(device), labels.to(device)\n",
        "                outputs = self.model(images)\n",
        "                loss = self.model.loss(outputs, labels)\n",
        "                self.test_loss.append(loss.item())\n",
        "                predicted_labels = torch.max(outputs.data, 1)[1]\n",
        "                self.predictions += predicted_labels.tolist()\n",
        "                self.truth_labels += labels.tolist()\n",
        "                correct_predictions += predicted_labels.eq(labels).sum().item()\n",
        "                total_predictions += labels.size(0)\n",
        "        \n",
        "        #Average test loss across test loader, calculate accuracy\n",
        "        self.test_loss = np.mean(self.test_loss)\n",
        "        accuracy = 100 * correct_predictions / total_predictions\n",
        "\n",
        "        #Print out accuracy, test loss, and training time\n",
        "        print(f'Accuracy of the network on the {total_predictions} test images: {accuracy:.2f}%. Final test loss: {self.test_loss:.2f}.')\n",
        "        print(f'The time taken to train the network was {int(self.train_time // 60)} mins {self.train_time % 60 :.0f} seconds')\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "    def print_predictions(self):\n",
        "        '''\n",
        "        For the first 6 samples in test_loader, print out the predicted labels made by the model, along with the images and\n",
        "        their truth labels\n",
        "        '''\n",
        "        #Reset self.test_loader as otherwise each new call shuffles the data, thus going out of sync with self.predictions\n",
        "        self.test_loader = torch.utils.data.DataLoader(self.test_set, batch_size=64, shuffle=False)\n",
        "\n",
        "        figure = plt.figure(figsize=(8, 7))\n",
        "        plt.title(\"The First 6 Samples And Their Predicted Labels\\n\\n\")\n",
        "        plt.axis(\"off\")\n",
        "        rows, cols = 2, 3\n",
        "\n",
        "        #Iterate through the first 6 images in test loader and print them, labelling them with their truth labels and model predictions\n",
        "        for inputs, _ in self.test_loader:\n",
        "            for i in range(1, cols * rows + 1):\n",
        "                image  = inputs[i-1].cpu().numpy()\n",
        "                image = image.T\n",
        "                figure.add_subplot(rows, cols, i)\n",
        "                plt.title(f\"predicted label: {self.labels_map[self.predictions[i-1]]}\\ntruth label: {self.labels_map[self.truth_labels[i-1]]}\")\n",
        "                plt.axis(\"off\")\n",
        "                plt.imshow(image.squeeze(), cmap=\"gray\")\n",
        "            break\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def report(self):\n",
        "        '''\n",
        "        Create, print, and display plot of confusion matrix. Print out various performance metrics and a classification report.\n",
        "        '''\n",
        "\n",
        "        #Create and print confusion matrix\n",
        "        conf_matrix = confusion_matrix(self.truth_labels, self.predictions)\n",
        "        print(\"Confusion Matrix\\n\")\n",
        "        print(conf_matrix)\n",
        "        #Plot heatmap of conf matrix\n",
        "        fig, ax = plt.subplots(figsize=(10, 8))\n",
        "        sns.heatmap(conf_matrix, annot=False, fmt='g', cmap='Blues', xticklabels=self.labels_map.values(), yticklabels=self.labels_map.values())\n",
        "        plt.title(\"Confusion Matrix\")\n",
        "\n",
        "        #Print out various metrics \n",
        "        print('\\nAccuracy: {:.2f}%\\n'.format(100*accuracy_score(self.truth_labels, self.predictions)))\n",
        "\n",
        "        print('Micro Precision: {:.2f}'.format(precision_score(self.truth_labels, self.predictions, average='micro')))\n",
        "        print('Micro Recall: {:.2f}'.format(recall_score(self.truth_labels, self.predictions, average='micro')))\n",
        "        print('Micro F1-score: {:.2f}\\n'.format(f1_score(self.truth_labels, self.predictions, average='micro')))\n",
        "\n",
        "        print('Macro Precision: {:.2f}'.format(precision_score(self.truth_labels, self.predictions, average='macro')))\n",
        "        print('Macro Recall: {:.2f}'.format(recall_score(self.truth_labels, self.predictions, average='macro')))\n",
        "        print('Macro F1-score: {:.2f}\\n'.format(f1_score(self.truth_labels, self.predictions, average='macro')))\n",
        "\n",
        "        print('Weighted Precision: {:.2f}'.format(precision_score(self.truth_labels, self.predictions, average='weighted')))\n",
        "        print('Weighted Recall: {:.2f}'.format(recall_score(self.truth_labels, self.predictions, average='weighted')))\n",
        "        print('Weighted F1-score: {:.2f}'.format(f1_score(self.truth_labels, self.predictions, average='weighted')))\n",
        "\n",
        "        #Print out classification report\n",
        "        print('\\nClassification Report\\n')\n",
        "        print(classification_report(self.truth_labels, self.predictions, target_names=[str(class_name) for class_name in self.labels_map.values()]))\n",
        "\n",
        "\n",
        "#Define hyperparameter map for hyperparameter tuning\n",
        "hyperparam_map = {'Learning Rate Scheduler': ['StepLR', 'ExponentialLR'],\n",
        "                'Activation function': ['relu', 'leaky_relu', 'elu'],\n",
        "                'Optimiser': [torch.optim.SGD, torch.optim.RMSprop, torch.optim.Adagrad],\n",
        "                'Batch Normalisation': [True, False],\n",
        "                'Regularisation': [(0.0001, 0), (0, 0.0001), (0.0001, 0.0001)],\n",
        "                'Dropout': [True, False]\n",
        "                }\n",
        "\n",
        "#Initialise baseline hyperparameter dictionary\n",
        "baseline_hyperparams = {'Learning Rate Scheduler': 'StepLR',\n",
        "                        'Activation function': 'relu',\n",
        "                        'Optimiser': torch.optim.SGD,\n",
        "                        'Batch Normalisation': True,\n",
        "                        'Regularisation': (0.0001, 0.0001),\n",
        "                        'Dropout': True\n",
        "                        }\n",
        "\n",
        "#Initialise CNN hyperparameter dictionary\n",
        "CNN_exclusive_baseline_hyperparams = {'out_channels_1':32, \n",
        "                                      'out_channels_2':64,\n",
        "                                      'conv_kernel_size':2,\n",
        "                                      'pool_kernel_size':2,\n",
        "                                      'fcl size': 50}\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hyperparam_dic, layer_1_size=600, layer_2_size=400, layer_3_size=200, learning_rate=0.1):\n",
        "        super(MLP, self).__init__()\n",
        "        self.name = 'MLP'\n",
        "        self.hyperparam_dic = hyperparam_dic\n",
        "        self.l1_lambda = baseline_hyperparams['Regularisation'][0]\n",
        "        self.l2_lambda = baseline_hyperparams['Regularisation'][1]\n",
        "        \n",
        "        self.FCL1 = nn.Linear(28*28, layer_1_size) #Input size\n",
        "        self.FCL2 = nn.Linear(layer_1_size, layer_2_size)\n",
        "        self.FCL3 = nn.Linear(layer_2_size, layer_3_size)\n",
        "        self.FCL4 = nn.Linear(layer_3_size, 47) #Number of classes   \n",
        "        \n",
        "        if hyperparam_dic['Activation function'] == 'relu':\n",
        "          self.activation = nn.ReLU()        \n",
        "        elif hyperparam_dic['Activation function'] == 'leaky_relu':\n",
        "          self.activation = nn.LeakyReLU()\n",
        "        elif hyperparam_dic['Activation function'] == 'elu':\n",
        "          self.activation = nn.ELU()\n",
        "\n",
        "        if self.hyperparam_dic['Batch Normalisation'] == True:\n",
        "          self.bn1 = nn.BatchNorm1d(layer_1_size)\n",
        "          self.bn2 = nn.BatchNorm1d(layer_2_size)\n",
        "          self.bn3 = nn.BatchNorm1d(layer_3_size)\n",
        "         \n",
        "        if self.hyperparam_dic['Dropout'] == True:\n",
        "          self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "        # Define hyperparameters\n",
        "        self.optimiser = hyperparam_dic['Optimiser'](self.parameters(), lr=learning_rate)\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        if hyperparam_dic['Learning Rate Scheduler'] == 'StepLR':\n",
        "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=1, gamma=0.2)\n",
        "\n",
        "        elif hyperparam_dic['Learning Rate Scheduler'] == 'ExponentialLR':\n",
        "            self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimiser, gamma=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Layer 1\n",
        "        out = self.FCL1(x)\n",
        "        if self.hyperparam_dic['Batch Normalisation'] == True:\n",
        "            out = self.bn1(out)\n",
        "        out = self.activation(out)\n",
        "        if self.hyperparam_dic['Dropout'] == True:\n",
        "            out = self.dropout(out)\n",
        "\n",
        "        #Layer 2 \n",
        "        out = self.FCL2(out)\n",
        "        if self.hyperparam_dic['Batch Normalisation'] == True:\n",
        "            out = self.bn2(out)\n",
        "        out = self.activation(out)\n",
        "        if self.hyperparam_dic['Dropout'] == True:\n",
        "            out = self.dropout(out)\n",
        "        \n",
        "        #Layer 3\n",
        "        out = self.FCL3(out)\n",
        "        if self.hyperparam_dic['Batch Normalisation'] == True:\n",
        "            out = self.bn3(out)\n",
        "        out = self.activation(out)\n",
        "        if self.hyperparam_dic['Dropout'] == True:\n",
        "            out = self.dropout(out)\n",
        "\n",
        "        #Output layer\n",
        "        out = self.FCL4(out)\n",
        "\n",
        "        return out \n",
        "    \n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, hyperparam_dic, CNN_exclusive_baseline_hyperparams, learning_rate=0.1,):\n",
        "        super(CNN, self).__init__()\n",
        "        self.name = 'CNN'\n",
        "        self.hyperparam_dic = hyperparam_dic\n",
        "        self.CNN_exclusive_baseline_hyperparams = CNN_exclusive_baseline_hyperparams\n",
        "        input_shape = (1, 28, 28)     #Input = list(np.shape(train_set[0][0]))\n",
        "        \n",
        "        self.l1_lambda = baseline_hyperparams['Regularisation'][0]\n",
        "        self.l2_lambda = baseline_hyperparams['Regularisation'][1]\n",
        "\n",
        "        if hyperparam_dic['Activation function'] == 'relu':\n",
        "          self.activation = nn.ReLU()        \n",
        "        elif hyperparam_dic['Activation function'] == 'leaky_relu':\n",
        "          self.activation = nn.LeakyReLU()\n",
        "        elif hyperparam_dic['Activation function'] == 'elu':\n",
        "          self.activation = nn.ELU()\n",
        "\n",
        "        if hyperparam_dic['Dropout'] == True:\n",
        "          self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "        # Define the convolution and pooling kernel size.\n",
        "        self.conv_kernel_size = CNN_exclusive_baseline_hyperparams['conv_kernel_size']\n",
        "        self.pool_kernel_size = CNN_exclusive_baseline_hyperparams['pool_kernel_size']\n",
        "\n",
        "        # Define layers\n",
        "        self.out_channels_1 = CNN_exclusive_baseline_hyperparams['out_channels_1']\n",
        "        self.in_channels_2 = self.out_channels_1\n",
        "        self.out_channels_2 = CNN_exclusive_baseline_hyperparams['out_channels_2']\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(input_shape[0], self.out_channels_1, self.conv_kernel_size)\n",
        "        self.conv2 = nn.Conv2d(self.in_channels_2, self.out_channels_2, self.conv_kernel_size)\n",
        "        self.pool = nn.MaxPool2d(self.pool_kernel_size)\n",
        "\n",
        "        self.fcl1_size = CNN_exclusive_baseline_hyperparams['fcl size']\n",
        "        \n",
        "        if hyperparam_dic['Batch Normalisation'] == True:\n",
        "          self.bn1 = nn.BatchNorm2d(self.out_channels_1)\n",
        "          self.bn2 = nn.BatchNorm2d(self.out_channels_2)\n",
        "          self.bn3 = nn.BatchNorm1d(self.fcl1_size)\n",
        "\n",
        "        #Calculate the output shape('num_flat_features') after convolutions and poolings\n",
        "        channels = input_shape[0]\n",
        "        width = input_shape[1]\n",
        "        height = input_shape[2]\n",
        "        \n",
        "        width =  width - (self.conv_kernel_size-1)\n",
        "        height = height - (self.conv_kernel_size-1)\n",
        "        channels = self.out_channels_1\n",
        "        \n",
        "        width = math.floor(width /self.pool_kernel_size)\n",
        "        height = math.floor(height /self.pool_kernel_size)\n",
        "        channels = channels\n",
        "        \n",
        "        width =  width - (self.conv_kernel_size-1)\n",
        "        height = height - (self.conv_kernel_size-1)\n",
        "        channels = self.out_channels_2\n",
        "        \n",
        "        width = math.floor(width /self.pool_kernel_size)\n",
        "        height = math.floor(height /self.pool_kernel_size)\n",
        "        channels = channels\n",
        "        \n",
        "        self.num_flat_features = width * height * channels\n",
        "\n",
        "        #Define the last fully connected layers\n",
        "        self.FCL1 = nn.Linear(self.num_flat_features, self.fcl1_size)\n",
        "        self.FCL2 = nn.Linear(self.fcl1_size, out_features=47)\n",
        "\n",
        "        # Define hyperparameters\n",
        "        self.optimiser = hyperparam_dic['Optimiser'](self.parameters(), lr=learning_rate)\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        if hyperparam_dic['Learning Rate Scheduler'] == 'StepLR':\n",
        "            self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimiser, step_size=1, gamma=0.2)\n",
        "\n",
        "        elif hyperparam_dic['Learning Rate Scheduler'] == 'ExponentialLR':\n",
        "            self.scheduler = torch.optim.lr_scheduler.ExponentialLR(self.optimiser, gamma=0.2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #Layer 1\n",
        "        out = self.conv1(x)                 \n",
        "        if self.hyperparam_dic['Batch Normalisation'] == True:\n",
        "            out = self.bn1(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.pool(out)                    \n",
        "        if self.hyperparam_dic['Dropout'] == True:\n",
        "          out = self.dropout(out)\n",
        "\n",
        "        #Layer 2\n",
        "        out = self.conv2(out)                  \n",
        "        if self.hyperparam_dic['Batch Normalisation'] == True:\n",
        "            out = self.bn2(out)\n",
        "        out = self.activation(out)\n",
        "        out =  self.pool(out)                      \n",
        "        if self.hyperparam_dic['Dropout'] == True:\n",
        "          out = self.dropout(out)\n",
        "\n",
        "        #Layer 3  \n",
        "        out = out.reshape(-1, self.num_flat_features)                \n",
        "        out = self.FCL1(out) \n",
        "        if self.hyperparam_dic['Batch Normalisation'] == True:\n",
        "            out = self.bn3(out)\n",
        "        out = self.activation(out)\n",
        "        if self.hyperparam_dic['Dropout'] == True:\n",
        "          out = self.dropout(out)\n",
        "        \n",
        "        #Output layer\n",
        "        out = self.FCL2(out)\n",
        "        return out\n",
        "\n",
        "def reset_weights(model):\n",
        "    if isinstance(model, nn.Conv2d) or isinstance(model, nn.Linear):\n",
        "        model.reset_parameters()\n",
        "\n",
        "def explore_w_kfold(model_choice, hyperparam_map, baseline_hyperparams, CNN_exclusive_baseline_hyperparams, n_epochs=5, k=5, Shuffle=True, batch_size=64):\n",
        "    '''\n",
        "    Systematically explore hyperparameter combinations, updating the baseline model if a new combination returns a better accuracy. \n",
        "    Use cross validation with k=5, training each training fold for n_epochs and then testing using the validation fold.\n",
        "    '''\n",
        "    \n",
        "    #Print out current baseline\n",
        "    print(\"Baseline Hyperparameters:\")\n",
        "    print(\"Model Choice:\", model_choice.__name__)\n",
        "    print(\"{\")\n",
        "    for key, value in baseline_hyperparams.items():\n",
        "        print(f\"{key}: {value}\")   \n",
        "    print(\"}\\n\")\n",
        "\n",
        "    #Initialise baseline models\n",
        "    if model_choice == MLP:\n",
        "      baseline_NN = MLP(baseline_hyperparams).to(device)\n",
        "\n",
        "    if model_choice == CNN:\n",
        "      baseline_NN = CNN(baseline_hyperparams, CNN_exclusive_baseline_hyperparams).to(device)\n",
        "    \n",
        "    #Initialise, train, and test baseline\n",
        "    baseline = Model(baseline_NN, train_set, test_set)\n",
        "    baseline.train(n_epochs, verbose=False)\n",
        "    baseline_accuracy = baseline.test()\n",
        "\n",
        "    for hyperparam in hyperparam_map:\n",
        "\n",
        "        #Create new model dictionary\n",
        "        new_model_hyperparams = baseline_hyperparams.copy()\n",
        "        options = hyperparam_map[hyperparam]\n",
        "        options.remove(baseline_hyperparams[hyperparam])\n",
        "\n",
        "        #Search through all options in the hyperparameter map, excluding the current baseline option\n",
        "        for option in options:\n",
        "            new_model_hyperparams[hyperparam] = option\n",
        "        \n",
        "            print(\"\\nTesting:\")\n",
        "            print(\"Model Choice\", model_choice.__name__)\n",
        "            print(\"{\")\n",
        "            for key, value in new_model_hyperparams.items():\n",
        "                print(f\"{key}: {value}\")   \n",
        "            print(\"}\")\n",
        "\n",
        "            kfold = KFold(n_splits=k, shuffle=Shuffle)\n",
        "            accuracy_per_fold = []\n",
        "\n",
        "            #Perform k-fold cross validation\n",
        "            for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_set)):\n",
        "                print(\"Current Fold:\", fold_idx + 1)\n",
        "                train_loader = DataLoader(train_set, batch_size=batch_size, sampler=SubsetRandomSampler(train_idx))\n",
        "                val_loader = DataLoader(train_set, batch_size=batch_size, sampler=SubsetRandomSampler(val_idx))\n",
        "                if model_choice == MLP:\n",
        "                    model = MLP(new_model_hyperparams).to(device)\n",
        "                if model_choice == CNN:\n",
        "                    model = CNN(new_model_hyperparams, CNN_exclusive_baseline_hyperparams).to(device)\n",
        "                #NN used to refer to either model \n",
        "                NN = Model(model, train_loader.dataset, test_set)\n",
        "                NN.train_loader = train_loader\n",
        "                NN.test_loader = val_loader\n",
        "                #Train and test new model\n",
        "                NN.train(n_epochs, verbose=False) \n",
        "                new_model_accuracy = NN.test() \n",
        "                accuracy_per_fold.append(new_model_accuracy)\n",
        "                model.apply(reset_weights)\n",
        "\n",
        "\n",
        "            new_model_accuracy = np.mean(accuracy_per_fold) \n",
        "            print(f\"New model average accuracy: {new_model_accuracy:.2f}%\")\n",
        "            #Compare new model accuracy to current baseline; if better, update baseline\n",
        "            if new_model_accuracy > baseline_accuracy:\n",
        "                print(\"Success! Improvement on the previous combination of hyper-parameters.\")\n",
        "                baseline_hyperparams[hyperparam] = option\n",
        "                baseline_accuracy = new_model_accuracy\n",
        "                baseline = model\n",
        "\n",
        "    print(f\"\\nFinal hyperparameter combination, with {baseline_accuracy:.2f}% accuracy:\")\n",
        "    print(\"{\")\n",
        "    for key, value in baseline_hyperparams.items():\n",
        "        print(f\"{key}: {value}\")  \n",
        "    print(\"}\")\n",
        "\n",
        "    return baseline\n",
        "\n",
        "\n",
        "'''\n",
        "Use the explore_w_kfold function to systematically find the best hyperparameter combinations for each model. We returned the following combinations.\n",
        "'''\n",
        "#best_mlp = explore_w_kfold(MLP, hyperparam_map, baseline_hyperparams, CNN_exclusive_baseline_hyperparams, n_epochs=5, k=5, Shuffle=True, batch_size=64)\n",
        "#best_cnn = explore_w_kfold(CNN, hyperparam_map, baseline_hyperparams, CNN_exclusive_baseline_hyperparams, n_epochs=5, k=3, Shuffle=True, batch_size=64)\n",
        "\n",
        "MLP_best_hyperparams = {\n",
        "'Learning Rate Scheduler': 'StepLR',\n",
        "'Activation function': 'relu',\n",
        "'Optimiser': torch.optim.SGD,\n",
        "'Batch Normalisation': True,\n",
        "'Regularisation': (0.0001, 0.0001),\n",
        "'Dropout': False\n",
        "}\n",
        "\n",
        "CNN_best_hyperparams = {\n",
        "'Learning Rate Scheduler': 'StepLR',\n",
        "'Activation function': 'elu',\n",
        "'Optimiser': torch.optim.Adagrad,\n",
        "'Batch Normalisation': True,\n",
        "'Regularisation': (0.0001, 0),\n",
        "'Dropout': False\n",
        "}\n",
        "\n",
        "best_mlp = MLP(MLP_best_hyperparams).to(device)\n",
        "best_mlp_model = Model(best_mlp, train_set, test_set)\n",
        "best_mlp_model.describe_dataset(train_set)\n",
        "best_mlp_model.train(n_epochs=10)\n",
        "# best_mlp_model.plot()\n",
        "# best_mlp_model.test()\n",
        "# best_mlp_model.print_predictions()\n",
        "# best_mlp_model.report()\n",
        "\n",
        "# best_cnn = CNN(CNN_best_hyperparams, CNN_exclusive_baseline_hyperparams).to(device)\n",
        "# best_cnn_model = Model(best_cnn, train_set, test_set)\n",
        "# best_cnn_model.describe_dataset(test_set)\n",
        "# best_cnn_model.train(n_epochs=10)\n",
        "# best_cnn_model.plot()\n",
        "# best_cnn_model.test()\n",
        "# best_cnn_model.print_predictions()\n",
        "# best_mlp_model.report()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWHkMBoEgaFb"
      },
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mb8cvrdYIY8Y",
        "outputId": "616219d7-cdbd-4b43-8349-1daa76ccf457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Step [100/1763], Loss: 2.8651\n",
            "Epoch [1/50], Step [200/1763], Loss: 2.3685\n",
            "Epoch [1/50], Step [300/1763], Loss: 2.2689\n",
            "Epoch [1/50], Step [400/1763], Loss: 2.4977\n",
            "Epoch [1/50], Step [500/1763], Loss: 2.3928\n",
            "Epoch [1/50], Step [600/1763], Loss: 1.9627\n",
            "Epoch [1/50], Step [700/1763], Loss: 2.1782\n",
            "Epoch [1/50], Step [800/1763], Loss: 1.9875\n",
            "Epoch [1/50], Step [900/1763], Loss: 2.2550\n",
            "Epoch [1/50], Step [1000/1763], Loss: 1.6382\n",
            "Epoch [1/50], Step [1100/1763], Loss: 1.9800\n",
            "Epoch [1/50], Step [1200/1763], Loss: 1.8016\n",
            "Epoch [1/50], Step [1300/1763], Loss: 1.7735\n",
            "Epoch [1/50], Step [1400/1763], Loss: 1.7234\n",
            "Epoch [1/50], Step [1500/1763], Loss: 1.5972\n",
            "Epoch [1/50], Step [1600/1763], Loss: 1.4861\n",
            "Epoch [1/50], Step [1700/1763], Loss: 1.5909\n",
            "Epoch [2/50], Step [100/1763], Loss: 1.3739\n",
            "Epoch [2/50], Step [200/1763], Loss: 1.4970\n",
            "Epoch [2/50], Step [300/1763], Loss: 1.5019\n",
            "Epoch [2/50], Step [400/1763], Loss: 1.2472\n",
            "Epoch [2/50], Step [500/1763], Loss: 1.4560\n",
            "Epoch [2/50], Step [600/1763], Loss: 1.4900\n",
            "Epoch [2/50], Step [700/1763], Loss: 1.2673\n",
            "Epoch [2/50], Step [800/1763], Loss: 1.2040\n",
            "Epoch [2/50], Step [900/1763], Loss: 1.3570\n",
            "Epoch [2/50], Step [1000/1763], Loss: 1.1152\n",
            "Epoch [2/50], Step [1100/1763], Loss: 1.2172\n",
            "Epoch [2/50], Step [1200/1763], Loss: 1.2111\n",
            "Epoch [2/50], Step [1300/1763], Loss: 1.2496\n",
            "Epoch [2/50], Step [1400/1763], Loss: 1.4412\n",
            "Epoch [2/50], Step [1500/1763], Loss: 1.1749\n",
            "Epoch [2/50], Step [1600/1763], Loss: 1.2433\n",
            "Epoch [2/50], Step [1700/1763], Loss: 1.2224\n",
            "Epoch [3/50], Step [100/1763], Loss: 1.0568\n",
            "Epoch [3/50], Step [200/1763], Loss: 1.1091\n",
            "Epoch [3/50], Step [300/1763], Loss: 1.1796\n",
            "Epoch [3/50], Step [400/1763], Loss: 1.0518\n",
            "Epoch [3/50], Step [500/1763], Loss: 1.1215\n",
            "Epoch [3/50], Step [600/1763], Loss: 1.0565\n",
            "Epoch [3/50], Step [700/1763], Loss: 1.2136\n",
            "Epoch [3/50], Step [800/1763], Loss: 1.0044\n",
            "Epoch [3/50], Step [900/1763], Loss: 1.0621\n",
            "Epoch [3/50], Step [1000/1763], Loss: 1.1942\n",
            "Epoch [3/50], Step [1100/1763], Loss: 1.0894\n",
            "Epoch [3/50], Step [1200/1763], Loss: 1.2427\n",
            "Epoch [3/50], Step [1300/1763], Loss: 1.1492\n",
            "Epoch [3/50], Step [1400/1763], Loss: 1.1949\n",
            "Epoch [3/50], Step [1500/1763], Loss: 1.0169\n",
            "Epoch [3/50], Step [1600/1763], Loss: 1.0605\n",
            "Epoch [3/50], Step [1700/1763], Loss: 1.1344\n",
            "Epoch [4/50], Step [100/1763], Loss: 1.0201\n",
            "Epoch [4/50], Step [200/1763], Loss: 1.0400\n",
            "Epoch [4/50], Step [300/1763], Loss: 1.3796\n",
            "Epoch [4/50], Step [400/1763], Loss: 0.9643\n",
            "Epoch [4/50], Step [500/1763], Loss: 1.0602\n",
            "Epoch [4/50], Step [600/1763], Loss: 0.9754\n",
            "Epoch [4/50], Step [700/1763], Loss: 1.2735\n",
            "Epoch [4/50], Step [800/1763], Loss: 1.1017\n",
            "Epoch [4/50], Step [900/1763], Loss: 1.0038\n",
            "Epoch [4/50], Step [1000/1763], Loss: 1.0523\n",
            "Epoch [4/50], Step [1100/1763], Loss: 1.1300\n",
            "Epoch [4/50], Step [1200/1763], Loss: 1.0537\n",
            "Epoch [4/50], Step [1300/1763], Loss: 1.0292\n",
            "Epoch [4/50], Step [1400/1763], Loss: 1.0633\n",
            "Epoch [4/50], Step [1500/1763], Loss: 1.1010\n",
            "Epoch [4/50], Step [1600/1763], Loss: 1.0397\n",
            "Epoch [4/50], Step [1700/1763], Loss: 1.0053\n",
            "Epoch [5/50], Step [100/1763], Loss: 1.2077\n",
            "Epoch [5/50], Step [200/1763], Loss: 1.0695\n",
            "Epoch [5/50], Step [300/1763], Loss: 1.0091\n",
            "Epoch [5/50], Step [400/1763], Loss: 0.9342\n",
            "Epoch [5/50], Step [500/1763], Loss: 1.1173\n",
            "Epoch [5/50], Step [600/1763], Loss: 1.1337\n",
            "Epoch [5/50], Step [700/1763], Loss: 1.1861\n",
            "Epoch [5/50], Step [800/1763], Loss: 1.0200\n",
            "Epoch [5/50], Step [900/1763], Loss: 1.0959\n",
            "Epoch [5/50], Step [1000/1763], Loss: 1.0064\n",
            "Epoch [5/50], Step [1100/1763], Loss: 1.0221\n",
            "Epoch [5/50], Step [1200/1763], Loss: 1.0566\n",
            "Epoch [5/50], Step [1300/1763], Loss: 1.0527\n",
            "Epoch [5/50], Step [1400/1763], Loss: 1.0628\n",
            "Epoch [5/50], Step [1500/1763], Loss: 1.1222\n",
            "Epoch [5/50], Step [1600/1763], Loss: 1.0924\n",
            "Epoch [5/50], Step [1700/1763], Loss: 0.9061\n",
            "Epoch [6/50], Step [100/1763], Loss: 1.1174\n",
            "Epoch [6/50], Step [200/1763], Loss: 1.0970\n",
            "Epoch [6/50], Step [300/1763], Loss: 1.0308\n",
            "Epoch [6/50], Step [400/1763], Loss: 1.0888\n",
            "Epoch [6/50], Step [500/1763], Loss: 1.2529\n",
            "Epoch [6/50], Step [600/1763], Loss: 1.3126\n",
            "Epoch [6/50], Step [700/1763], Loss: 0.9813\n",
            "Epoch [6/50], Step [800/1763], Loss: 1.0972\n",
            "Epoch [6/50], Step [900/1763], Loss: 1.3094\n",
            "Epoch [6/50], Step [1000/1763], Loss: 1.0558\n",
            "Epoch [6/50], Step [1100/1763], Loss: 1.0895\n",
            "Epoch [6/50], Step [1200/1763], Loss: 1.0472\n",
            "Epoch [6/50], Step [1300/1763], Loss: 1.1209\n",
            "Epoch [6/50], Step [1400/1763], Loss: 1.0310\n",
            "Epoch [6/50], Step [1500/1763], Loss: 1.0723\n",
            "Epoch [6/50], Step [1600/1763], Loss: 1.0409\n",
            "Epoch [6/50], Step [1700/1763], Loss: 1.0841\n",
            "Epoch [7/50], Step [100/1763], Loss: 1.0992\n",
            "Epoch [7/50], Step [200/1763], Loss: 1.1924\n",
            "Epoch [7/50], Step [300/1763], Loss: 1.0840\n",
            "Epoch [7/50], Step [400/1763], Loss: 1.2427\n",
            "Epoch [7/50], Step [500/1763], Loss: 1.0579\n",
            "Epoch [7/50], Step [600/1763], Loss: 0.9728\n",
            "Epoch [7/50], Step [700/1763], Loss: 0.9668\n",
            "Epoch [7/50], Step [800/1763], Loss: 1.1058\n",
            "Epoch [7/50], Step [900/1763], Loss: 1.1273\n",
            "Epoch [7/50], Step [1000/1763], Loss: 1.2903\n",
            "Epoch [7/50], Step [1100/1763], Loss: 1.1519\n",
            "Epoch [7/50], Step [1200/1763], Loss: 0.9897\n",
            "Epoch [7/50], Step [1300/1763], Loss: 1.1407\n",
            "Epoch [7/50], Step [1400/1763], Loss: 0.9462\n",
            "Epoch [7/50], Step [1500/1763], Loss: 1.1881\n",
            "Epoch [7/50], Step [1600/1763], Loss: 1.0050\n",
            "Epoch [7/50], Step [1700/1763], Loss: 1.0480\n",
            "Epoch [8/50], Step [100/1763], Loss: 1.0842\n",
            "Epoch [8/50], Step [200/1763], Loss: 1.0246\n",
            "Epoch [8/50], Step [300/1763], Loss: 1.0920\n",
            "Epoch [8/50], Step [400/1763], Loss: 1.5967\n",
            "Epoch [8/50], Step [500/1763], Loss: 1.1644\n",
            "Epoch [8/50], Step [600/1763], Loss: 0.9760\n",
            "Epoch [8/50], Step [700/1763], Loss: 0.9603\n",
            "Epoch [8/50], Step [800/1763], Loss: 1.1291\n",
            "Epoch [8/50], Step [900/1763], Loss: 0.9559\n",
            "Epoch [8/50], Step [1000/1763], Loss: 1.0238\n",
            "Epoch [8/50], Step [1100/1763], Loss: 1.0472\n",
            "Epoch [8/50], Step [1200/1763], Loss: 1.2058\n",
            "Epoch [8/50], Step [1300/1763], Loss: 0.9633\n",
            "Epoch [8/50], Step [1400/1763], Loss: 1.0472\n",
            "Epoch [8/50], Step [1500/1763], Loss: 1.0972\n",
            "Epoch [8/50], Step [1600/1763], Loss: 1.0939\n",
            "Epoch [8/50], Step [1700/1763], Loss: 1.0280\n",
            "Epoch [9/50], Step [100/1763], Loss: 1.0353\n",
            "Epoch [9/50], Step [200/1763], Loss: 0.9643\n",
            "Epoch [9/50], Step [300/1763], Loss: 1.1201\n",
            "Epoch [9/50], Step [400/1763], Loss: 1.0107\n",
            "Epoch [9/50], Step [500/1763], Loss: 1.0872\n",
            "Epoch [9/50], Step [600/1763], Loss: 1.1925\n",
            "Epoch [9/50], Step [700/1763], Loss: 1.1495\n",
            "Epoch [9/50], Step [800/1763], Loss: 0.9601\n",
            "Epoch [9/50], Step [900/1763], Loss: 1.1551\n",
            "Epoch [9/50], Step [1000/1763], Loss: 1.1311\n",
            "Epoch [9/50], Step [1100/1763], Loss: 1.0426\n",
            "Epoch [9/50], Step [1200/1763], Loss: 1.1142\n",
            "Epoch [9/50], Step [1300/1763], Loss: 1.0211\n",
            "Epoch [9/50], Step [1400/1763], Loss: 1.0028\n",
            "Epoch [9/50], Step [1500/1763], Loss: 1.0330\n",
            "Epoch [9/50], Step [1600/1763], Loss: 1.0772\n",
            "Epoch [9/50], Step [1700/1763], Loss: 1.2180\n",
            "Epoch [10/50], Step [100/1763], Loss: 1.1329\n",
            "Epoch [10/50], Step [200/1763], Loss: 1.2167\n",
            "Epoch [10/50], Step [300/1763], Loss: 1.0623\n",
            "Epoch [10/50], Step [400/1763], Loss: 1.0414\n",
            "Epoch [10/50], Step [500/1763], Loss: 1.1938\n",
            "Epoch [10/50], Step [600/1763], Loss: 1.0853\n",
            "Epoch [10/50], Step [700/1763], Loss: 1.0715\n",
            "Epoch [10/50], Step [800/1763], Loss: 0.9278\n",
            "Epoch [10/50], Step [900/1763], Loss: 1.1369\n",
            "Epoch [10/50], Step [1000/1763], Loss: 1.0752\n",
            "Epoch [10/50], Step [1100/1763], Loss: 1.0035\n",
            "Epoch [10/50], Step [1200/1763], Loss: 1.0715\n",
            "Epoch [10/50], Step [1300/1763], Loss: 1.1696\n",
            "Epoch [10/50], Step [1400/1763], Loss: 1.0704\n",
            "Epoch [10/50], Step [1500/1763], Loss: 1.2574\n",
            "Epoch [10/50], Step [1600/1763], Loss: 1.0030\n",
            "Epoch [10/50], Step [1700/1763], Loss: 1.0168\n",
            "Epoch [11/50], Step [100/1763], Loss: 1.0275\n",
            "Epoch [11/50], Step [200/1763], Loss: 1.1318\n",
            "Epoch [11/50], Step [300/1763], Loss: 1.2714\n",
            "Epoch [11/50], Step [400/1763], Loss: 1.2792\n",
            "Epoch [11/50], Step [500/1763], Loss: 1.1547\n",
            "Epoch [11/50], Step [600/1763], Loss: 0.9919\n",
            "Epoch [11/50], Step [700/1763], Loss: 0.9702\n",
            "Epoch [11/50], Step [800/1763], Loss: 1.0335\n",
            "Epoch [11/50], Step [900/1763], Loss: 1.1121\n",
            "Epoch [11/50], Step [1000/1763], Loss: 1.2044\n",
            "Epoch [11/50], Step [1100/1763], Loss: 1.0607\n",
            "Epoch [11/50], Step [1200/1763], Loss: 1.0682\n",
            "Epoch [11/50], Step [1300/1763], Loss: 1.0669\n",
            "Epoch [11/50], Step [1400/1763], Loss: 0.9930\n",
            "Epoch [11/50], Step [1500/1763], Loss: 1.3119\n",
            "Epoch [11/50], Step [1600/1763], Loss: 1.0985\n",
            "Epoch [11/50], Step [1700/1763], Loss: 1.0845\n",
            "Epoch [12/50], Step [100/1763], Loss: 0.9809\n",
            "Epoch [12/50], Step [200/1763], Loss: 0.9568\n",
            "Epoch [12/50], Step [300/1763], Loss: 1.0089\n",
            "Epoch [12/50], Step [400/1763], Loss: 1.0260\n",
            "Epoch [12/50], Step [500/1763], Loss: 1.0720\n",
            "Epoch [12/50], Step [600/1763], Loss: 1.5060\n",
            "Epoch [12/50], Step [700/1763], Loss: 1.1914\n",
            "Epoch [12/50], Step [800/1763], Loss: 1.1557\n",
            "Epoch [12/50], Step [900/1763], Loss: 1.1029\n",
            "Epoch [12/50], Step [1000/1763], Loss: 1.0753\n",
            "Epoch [12/50], Step [1100/1763], Loss: 1.0166\n",
            "Epoch [12/50], Step [1200/1763], Loss: 0.9837\n",
            "Epoch [12/50], Step [1300/1763], Loss: 1.0354\n",
            "Epoch [12/50], Step [1400/1763], Loss: 1.1721\n",
            "Epoch [12/50], Step [1500/1763], Loss: 1.0733\n",
            "Epoch [12/50], Step [1600/1763], Loss: 1.1270\n",
            "Epoch [12/50], Step [1700/1763], Loss: 1.1558\n",
            "Epoch [13/50], Step [100/1763], Loss: 1.1726\n",
            "Epoch [13/50], Step [200/1763], Loss: 1.0855\n",
            "Epoch [13/50], Step [300/1763], Loss: 1.1593\n",
            "Epoch [13/50], Step [400/1763], Loss: 0.9530\n",
            "Epoch [13/50], Step [500/1763], Loss: 1.1166\n",
            "Epoch [13/50], Step [600/1763], Loss: 1.0831\n",
            "Epoch [13/50], Step [700/1763], Loss: 1.1654\n",
            "Epoch [13/50], Step [800/1763], Loss: 1.1210\n",
            "Epoch [13/50], Step [900/1763], Loss: 1.2401\n",
            "Epoch [13/50], Step [1000/1763], Loss: 1.0477\n",
            "Epoch [13/50], Step [1100/1763], Loss: 1.0391\n",
            "Epoch [13/50], Step [1200/1763], Loss: 1.1026\n",
            "Epoch [13/50], Step [1300/1763], Loss: 1.0032\n",
            "Epoch [13/50], Step [1400/1763], Loss: 0.9354\n",
            "Epoch [13/50], Step [1500/1763], Loss: 1.0032\n",
            "Epoch [13/50], Step [1600/1763], Loss: 1.0752\n",
            "Epoch [13/50], Step [1700/1763], Loss: 1.0004\n",
            "Epoch [14/50], Step [100/1763], Loss: 1.0871\n",
            "Epoch [14/50], Step [200/1763], Loss: 0.9637\n",
            "Epoch [14/50], Step [300/1763], Loss: 1.0476\n",
            "Epoch [14/50], Step [400/1763], Loss: 0.9081\n",
            "Epoch [14/50], Step [500/1763], Loss: 1.0905\n",
            "Epoch [14/50], Step [600/1763], Loss: 1.2853\n",
            "Epoch [14/50], Step [700/1763], Loss: 1.1176\n",
            "Epoch [14/50], Step [800/1763], Loss: 1.0364\n",
            "Epoch [14/50], Step [900/1763], Loss: 1.1939\n",
            "Epoch [14/50], Step [1000/1763], Loss: 1.0101\n",
            "Epoch [14/50], Step [1100/1763], Loss: 1.0265\n",
            "Epoch [14/50], Step [1200/1763], Loss: 1.2805\n",
            "Epoch [14/50], Step [1300/1763], Loss: 1.3530\n",
            "Epoch [14/50], Step [1400/1763], Loss: 1.1556\n",
            "Epoch [14/50], Step [1500/1763], Loss: 0.9933\n",
            "Epoch [14/50], Step [1600/1763], Loss: 1.1125\n",
            "Epoch [14/50], Step [1700/1763], Loss: 1.1951\n",
            "Epoch [15/50], Step [100/1763], Loss: 1.2024\n",
            "Epoch [15/50], Step [200/1763], Loss: 1.0506\n",
            "Epoch [15/50], Step [300/1763], Loss: 1.0876\n",
            "Epoch [15/50], Step [400/1763], Loss: 1.0401\n",
            "Epoch [15/50], Step [500/1763], Loss: 1.0680\n",
            "Epoch [15/50], Step [600/1763], Loss: 1.2135\n",
            "Epoch [15/50], Step [700/1763], Loss: 1.1363\n",
            "Epoch [15/50], Step [800/1763], Loss: 1.2593\n",
            "Epoch [15/50], Step [900/1763], Loss: 1.0996\n",
            "Epoch [15/50], Step [1000/1763], Loss: 1.0836\n",
            "Epoch [15/50], Step [1100/1763], Loss: 1.2045\n",
            "Epoch [15/50], Step [1200/1763], Loss: 1.1024\n",
            "Epoch [15/50], Step [1300/1763], Loss: 1.0917\n",
            "Epoch [15/50], Step [1400/1763], Loss: 1.1892\n",
            "Epoch [15/50], Step [1500/1763], Loss: 1.1701\n",
            "Epoch [15/50], Step [1600/1763], Loss: 1.1641\n",
            "Epoch [15/50], Step [1700/1763], Loss: 1.0626\n",
            "Epoch [16/50], Step [100/1763], Loss: 1.0805\n",
            "Epoch [16/50], Step [200/1763], Loss: 0.9786\n",
            "Epoch [16/50], Step [300/1763], Loss: 1.0111\n",
            "Epoch [16/50], Step [400/1763], Loss: 1.1192\n",
            "Epoch [16/50], Step [500/1763], Loss: 1.0981\n",
            "Epoch [16/50], Step [600/1763], Loss: 1.1243\n",
            "Epoch [16/50], Step [700/1763], Loss: 1.0764\n",
            "Epoch [16/50], Step [800/1763], Loss: 1.0927\n",
            "Epoch [16/50], Step [900/1763], Loss: 0.9697\n",
            "Epoch [16/50], Step [1000/1763], Loss: 1.1201\n",
            "Epoch [16/50], Step [1100/1763], Loss: 1.0490\n",
            "Epoch [16/50], Step [1200/1763], Loss: 1.2671\n",
            "Epoch [16/50], Step [1300/1763], Loss: 0.9468\n",
            "Epoch [16/50], Step [1400/1763], Loss: 1.1795\n",
            "Epoch [16/50], Step [1500/1763], Loss: 1.0023\n",
            "Epoch [16/50], Step [1600/1763], Loss: 1.0454\n",
            "Epoch [16/50], Step [1700/1763], Loss: 1.0404\n",
            "Epoch [17/50], Step [100/1763], Loss: 1.0570\n",
            "Epoch [17/50], Step [200/1763], Loss: 1.2158\n",
            "Epoch [17/50], Step [300/1763], Loss: 1.1206\n",
            "Epoch [17/50], Step [400/1763], Loss: 1.1500\n",
            "Epoch [17/50], Step [500/1763], Loss: 1.0008\n",
            "Epoch [17/50], Step [600/1763], Loss: 0.9759\n",
            "Epoch [17/50], Step [700/1763], Loss: 1.0641\n",
            "Epoch [17/50], Step [800/1763], Loss: 1.0833\n",
            "Epoch [17/50], Step [900/1763], Loss: 1.1911\n",
            "Epoch [17/50], Step [1000/1763], Loss: 1.0715\n",
            "Epoch [17/50], Step [1100/1763], Loss: 1.1416\n",
            "Epoch [17/50], Step [1200/1763], Loss: 1.1101\n",
            "Epoch [17/50], Step [1300/1763], Loss: 1.0895\n",
            "Epoch [17/50], Step [1400/1763], Loss: 1.0675\n",
            "Epoch [17/50], Step [1500/1763], Loss: 0.9280\n",
            "Epoch [17/50], Step [1600/1763], Loss: 1.0034\n",
            "Epoch [17/50], Step [1700/1763], Loss: 1.1076\n",
            "Epoch [18/50], Step [100/1763], Loss: 0.9433\n",
            "Epoch [18/50], Step [200/1763], Loss: 1.1456\n",
            "Epoch [18/50], Step [300/1763], Loss: 1.1006\n",
            "Epoch [18/50], Step [400/1763], Loss: 1.1250\n",
            "Epoch [18/50], Step [500/1763], Loss: 1.2080\n",
            "Epoch [18/50], Step [600/1763], Loss: 1.1424\n",
            "Epoch [18/50], Step [700/1763], Loss: 1.0646\n",
            "Epoch [18/50], Step [800/1763], Loss: 0.9966\n",
            "Epoch [18/50], Step [900/1763], Loss: 1.0439\n",
            "Epoch [18/50], Step [1000/1763], Loss: 1.0544\n",
            "Epoch [18/50], Step [1100/1763], Loss: 1.1436\n",
            "Epoch [18/50], Step [1200/1763], Loss: 1.0033\n",
            "Epoch [18/50], Step [1300/1763], Loss: 1.0398\n",
            "Epoch [18/50], Step [1400/1763], Loss: 1.0642\n",
            "Epoch [18/50], Step [1500/1763], Loss: 0.9473\n",
            "Epoch [18/50], Step [1600/1763], Loss: 1.2414\n",
            "Epoch [18/50], Step [1700/1763], Loss: 1.0128\n",
            "Epoch [19/50], Step [100/1763], Loss: 1.0318\n",
            "Epoch [19/50], Step [200/1763], Loss: 0.9658\n",
            "Epoch [19/50], Step [300/1763], Loss: 1.0246\n",
            "Epoch [19/50], Step [400/1763], Loss: 1.3144\n",
            "Epoch [19/50], Step [500/1763], Loss: 1.0388\n",
            "Epoch [19/50], Step [600/1763], Loss: 1.0503\n",
            "Epoch [19/50], Step [700/1763], Loss: 1.0380\n",
            "Epoch [19/50], Step [800/1763], Loss: 1.0295\n",
            "Epoch [19/50], Step [900/1763], Loss: 0.9756\n",
            "Epoch [19/50], Step [1000/1763], Loss: 1.0792\n",
            "Epoch [19/50], Step [1100/1763], Loss: 1.0467\n",
            "Epoch [19/50], Step [1200/1763], Loss: 1.1190\n",
            "Epoch [19/50], Step [1300/1763], Loss: 1.0865\n",
            "Epoch [19/50], Step [1400/1763], Loss: 1.1378\n",
            "Epoch [19/50], Step [1500/1763], Loss: 1.2252\n",
            "Epoch [19/50], Step [1600/1763], Loss: 0.9683\n",
            "Epoch [19/50], Step [1700/1763], Loss: 1.1427\n",
            "Epoch [20/50], Step [100/1763], Loss: 0.9808\n",
            "Epoch [20/50], Step [200/1763], Loss: 1.0315\n",
            "Epoch [20/50], Step [300/1763], Loss: 1.0472\n",
            "Epoch [20/50], Step [400/1763], Loss: 1.0906\n",
            "Epoch [20/50], Step [500/1763], Loss: 1.0726\n",
            "Epoch [20/50], Step [600/1763], Loss: 1.1020\n",
            "Epoch [20/50], Step [700/1763], Loss: 1.1022\n",
            "Epoch [20/50], Step [800/1763], Loss: 1.1611\n",
            "Epoch [20/50], Step [900/1763], Loss: 1.0436\n",
            "Epoch [20/50], Step [1000/1763], Loss: 1.1184\n",
            "Epoch [20/50], Step [1100/1763], Loss: 0.9871\n",
            "Epoch [20/50], Step [1200/1763], Loss: 0.9496\n",
            "Epoch [20/50], Step [1300/1763], Loss: 1.0229\n",
            "Epoch [20/50], Step [1400/1763], Loss: 1.0244\n",
            "Epoch [20/50], Step [1500/1763], Loss: 1.0680\n",
            "Epoch [20/50], Step [1600/1763], Loss: 1.1154\n",
            "Epoch [20/50], Step [1700/1763], Loss: 1.0004\n",
            "Epoch [21/50], Step [100/1763], Loss: 1.2484\n",
            "Epoch [21/50], Step [200/1763], Loss: 1.4570\n",
            "Epoch [21/50], Step [300/1763], Loss: 1.0081\n",
            "Epoch [21/50], Step [400/1763], Loss: 1.0551\n",
            "Epoch [21/50], Step [500/1763], Loss: 0.9418\n",
            "Epoch [21/50], Step [600/1763], Loss: 1.0540\n",
            "Epoch [21/50], Step [700/1763], Loss: 1.0068\n",
            "Epoch [21/50], Step [800/1763], Loss: 1.0073\n",
            "Epoch [21/50], Step [900/1763], Loss: 1.0441\n",
            "Epoch [21/50], Step [1000/1763], Loss: 1.1318\n",
            "Epoch [21/50], Step [1100/1763], Loss: 1.2550\n",
            "Epoch [21/50], Step [1200/1763], Loss: 1.0162\n",
            "Epoch [21/50], Step [1300/1763], Loss: 0.9715\n",
            "Epoch [21/50], Step [1400/1763], Loss: 1.0908\n",
            "Epoch [21/50], Step [1500/1763], Loss: 1.0679\n",
            "Epoch [21/50], Step [1600/1763], Loss: 1.2254\n",
            "Epoch [21/50], Step [1700/1763], Loss: 1.1027\n",
            "Epoch [22/50], Step [100/1763], Loss: 0.9835\n",
            "Epoch [22/50], Step [200/1763], Loss: 1.3995\n",
            "Epoch [22/50], Step [300/1763], Loss: 1.0996\n",
            "Epoch [22/50], Step [400/1763], Loss: 1.1534\n",
            "Epoch [22/50], Step [500/1763], Loss: 1.0348\n",
            "Epoch [22/50], Step [600/1763], Loss: 1.0694\n",
            "Epoch [22/50], Step [700/1763], Loss: 1.0595\n",
            "Epoch [22/50], Step [800/1763], Loss: 1.1002\n",
            "Epoch [22/50], Step [900/1763], Loss: 1.0512\n",
            "Epoch [22/50], Step [1000/1763], Loss: 1.0206\n",
            "Epoch [22/50], Step [1100/1763], Loss: 1.1761\n",
            "Epoch [22/50], Step [1200/1763], Loss: 1.0434\n",
            "Epoch [22/50], Step [1300/1763], Loss: 1.1960\n",
            "Epoch [22/50], Step [1400/1763], Loss: 1.0124\n",
            "Epoch [22/50], Step [1500/1763], Loss: 1.1078\n",
            "Epoch [22/50], Step [1600/1763], Loss: 1.0711\n",
            "Epoch [22/50], Step [1700/1763], Loss: 1.1329\n",
            "Epoch [23/50], Step [100/1763], Loss: 1.1334\n",
            "Epoch [23/50], Step [200/1763], Loss: 1.1629\n",
            "Epoch [23/50], Step [300/1763], Loss: 1.1308\n",
            "Epoch [23/50], Step [400/1763], Loss: 1.0637\n",
            "Epoch [23/50], Step [500/1763], Loss: 1.1787\n",
            "Epoch [23/50], Step [600/1763], Loss: 1.2266\n",
            "Epoch [23/50], Step [700/1763], Loss: 1.1445\n",
            "Epoch [23/50], Step [800/1763], Loss: 1.2211\n",
            "Epoch [23/50], Step [900/1763], Loss: 1.2136\n",
            "Epoch [23/50], Step [1000/1763], Loss: 1.0910\n",
            "Epoch [23/50], Step [1100/1763], Loss: 1.2437\n",
            "Epoch [23/50], Step [1200/1763], Loss: 1.0355\n",
            "Epoch [23/50], Step [1300/1763], Loss: 1.0713\n",
            "Epoch [23/50], Step [1400/1763], Loss: 1.1579\n",
            "Epoch [23/50], Step [1500/1763], Loss: 1.0697\n",
            "Epoch [23/50], Step [1600/1763], Loss: 1.0085\n",
            "Epoch [23/50], Step [1700/1763], Loss: 1.0478\n",
            "Epoch [24/50], Step [100/1763], Loss: 0.9782\n",
            "Epoch [24/50], Step [200/1763], Loss: 1.1275\n",
            "Epoch [24/50], Step [300/1763], Loss: 1.0760\n",
            "Epoch [24/50], Step [400/1763], Loss: 1.1174\n",
            "Epoch [24/50], Step [500/1763], Loss: 0.9584\n",
            "Epoch [24/50], Step [600/1763], Loss: 1.1351\n",
            "Epoch [24/50], Step [700/1763], Loss: 1.1199\n",
            "Epoch [24/50], Step [800/1763], Loss: 1.2150\n",
            "Epoch [24/50], Step [900/1763], Loss: 1.0748\n",
            "Epoch [24/50], Step [1000/1763], Loss: 1.1541\n",
            "Epoch [24/50], Step [1100/1763], Loss: 1.0547\n",
            "Epoch [24/50], Step [1200/1763], Loss: 1.0474\n",
            "Epoch [24/50], Step [1300/1763], Loss: 1.0898\n",
            "Epoch [24/50], Step [1400/1763], Loss: 1.0292\n",
            "Epoch [24/50], Step [1500/1763], Loss: 1.0666\n",
            "Epoch [24/50], Step [1600/1763], Loss: 1.0451\n",
            "Epoch [24/50], Step [1700/1763], Loss: 1.0443\n",
            "Epoch [25/50], Step [100/1763], Loss: 1.0853\n",
            "Epoch [25/50], Step [200/1763], Loss: 1.3838\n",
            "Epoch [25/50], Step [300/1763], Loss: 1.2833\n",
            "Epoch [25/50], Step [400/1763], Loss: 1.1368\n",
            "Epoch [25/50], Step [500/1763], Loss: 1.0472\n",
            "Epoch [25/50], Step [600/1763], Loss: 0.9894\n",
            "Epoch [25/50], Step [700/1763], Loss: 1.1491\n",
            "Epoch [25/50], Step [800/1763], Loss: 1.1369\n",
            "Epoch [25/50], Step [900/1763], Loss: 1.0619\n",
            "Epoch [25/50], Step [1000/1763], Loss: 1.2042\n",
            "Epoch [25/50], Step [1100/1763], Loss: 1.2512\n",
            "Epoch [25/50], Step [1200/1763], Loss: 1.0298\n",
            "Epoch [25/50], Step [1300/1763], Loss: 0.9825\n",
            "Epoch [25/50], Step [1400/1763], Loss: 1.3618\n",
            "Epoch [25/50], Step [1500/1763], Loss: 1.1732\n",
            "Epoch [25/50], Step [1600/1763], Loss: 1.1715\n",
            "Epoch [25/50], Step [1700/1763], Loss: 1.0473\n",
            "Epoch [26/50], Step [100/1763], Loss: 1.0122\n",
            "Epoch [26/50], Step [200/1763], Loss: 1.0448\n",
            "Epoch [26/50], Step [300/1763], Loss: 1.0290\n",
            "Epoch [26/50], Step [400/1763], Loss: 1.1758\n",
            "Epoch [26/50], Step [500/1763], Loss: 1.0397\n",
            "Epoch [26/50], Step [600/1763], Loss: 0.9504\n",
            "Epoch [26/50], Step [700/1763], Loss: 0.8991\n",
            "Epoch [26/50], Step [800/1763], Loss: 1.1607\n",
            "Epoch [26/50], Step [900/1763], Loss: 1.1327\n",
            "Epoch [26/50], Step [1000/1763], Loss: 0.9636\n",
            "Epoch [26/50], Step [1100/1763], Loss: 1.1232\n",
            "Epoch [26/50], Step [1200/1763], Loss: 1.0859\n",
            "Epoch [26/50], Step [1300/1763], Loss: 1.2389\n",
            "Epoch [26/50], Step [1400/1763], Loss: 0.9958\n",
            "Epoch [26/50], Step [1500/1763], Loss: 1.1028\n",
            "Epoch [26/50], Step [1600/1763], Loss: 1.0143\n",
            "Epoch [26/50], Step [1700/1763], Loss: 1.0744\n",
            "Epoch [27/50], Step [100/1763], Loss: 1.0962\n",
            "Epoch [27/50], Step [200/1763], Loss: 1.2332\n",
            "Epoch [27/50], Step [300/1763], Loss: 1.2054\n",
            "Epoch [27/50], Step [400/1763], Loss: 1.1183\n",
            "Epoch [27/50], Step [500/1763], Loss: 0.9635\n",
            "Epoch [27/50], Step [600/1763], Loss: 1.1684\n",
            "Epoch [27/50], Step [700/1763], Loss: 1.1955\n",
            "Epoch [27/50], Step [800/1763], Loss: 1.3353\n",
            "Epoch [27/50], Step [900/1763], Loss: 1.0093\n",
            "Epoch [27/50], Step [1000/1763], Loss: 0.9594\n",
            "Epoch [27/50], Step [1100/1763], Loss: 1.0042\n",
            "Epoch [27/50], Step [1200/1763], Loss: 1.1861\n",
            "Epoch [27/50], Step [1300/1763], Loss: 1.1687\n",
            "Epoch [27/50], Step [1400/1763], Loss: 1.0165\n",
            "Epoch [27/50], Step [1500/1763], Loss: 1.2428\n",
            "Epoch [27/50], Step [1600/1763], Loss: 1.3986\n",
            "Epoch [27/50], Step [1700/1763], Loss: 1.0916\n",
            "Epoch [28/50], Step [100/1763], Loss: 1.0703\n",
            "Epoch [28/50], Step [200/1763], Loss: 1.0660\n",
            "Epoch [28/50], Step [300/1763], Loss: 1.1264\n",
            "Epoch [28/50], Step [400/1763], Loss: 1.0943\n",
            "Epoch [28/50], Step [500/1763], Loss: 0.9767\n",
            "Epoch [28/50], Step [600/1763], Loss: 1.0094\n",
            "Epoch [28/50], Step [700/1763], Loss: 1.1296\n",
            "Epoch [28/50], Step [800/1763], Loss: 1.1586\n",
            "Epoch [28/50], Step [900/1763], Loss: 1.0567\n",
            "Epoch [28/50], Step [1000/1763], Loss: 0.9981\n",
            "Epoch [28/50], Step [1100/1763], Loss: 1.1375\n",
            "Epoch [28/50], Step [1200/1763], Loss: 1.0237\n",
            "Epoch [28/50], Step [1300/1763], Loss: 1.3200\n",
            "Epoch [28/50], Step [1400/1763], Loss: 1.0420\n",
            "Epoch [28/50], Step [1500/1763], Loss: 0.9370\n",
            "Epoch [28/50], Step [1600/1763], Loss: 1.1984\n",
            "Epoch [28/50], Step [1700/1763], Loss: 1.1368\n",
            "Epoch [29/50], Step [100/1763], Loss: 1.0366\n",
            "Epoch [29/50], Step [200/1763], Loss: 1.1442\n",
            "Epoch [29/50], Step [300/1763], Loss: 1.2190\n",
            "Epoch [29/50], Step [400/1763], Loss: 1.1985\n",
            "Epoch [29/50], Step [500/1763], Loss: 1.2196\n",
            "Epoch [29/50], Step [600/1763], Loss: 1.0918\n",
            "Epoch [29/50], Step [700/1763], Loss: 1.1076\n",
            "Epoch [29/50], Step [800/1763], Loss: 1.1790\n",
            "Epoch [29/50], Step [900/1763], Loss: 1.2366\n",
            "Epoch [29/50], Step [1000/1763], Loss: 1.1197\n",
            "Epoch [29/50], Step [1100/1763], Loss: 1.2487\n",
            "Epoch [29/50], Step [1200/1763], Loss: 1.0216\n",
            "Epoch [29/50], Step [1300/1763], Loss: 1.1072\n",
            "Epoch [29/50], Step [1400/1763], Loss: 1.1502\n",
            "Epoch [29/50], Step [1500/1763], Loss: 1.1395\n",
            "Epoch [29/50], Step [1600/1763], Loss: 1.0167\n",
            "Epoch [29/50], Step [1700/1763], Loss: 0.9786\n",
            "Epoch [30/50], Step [100/1763], Loss: 1.2659\n",
            "Epoch [30/50], Step [200/1763], Loss: 1.1889\n",
            "Epoch [30/50], Step [300/1763], Loss: 1.1141\n",
            "Epoch [30/50], Step [400/1763], Loss: 1.1060\n",
            "Epoch [30/50], Step [500/1763], Loss: 1.0658\n",
            "Epoch [30/50], Step [600/1763], Loss: 1.1583\n",
            "Epoch [30/50], Step [700/1763], Loss: 1.1460\n",
            "Epoch [30/50], Step [800/1763], Loss: 1.2511\n",
            "Epoch [30/50], Step [900/1763], Loss: 1.0607\n",
            "Epoch [30/50], Step [1000/1763], Loss: 0.9839\n",
            "Epoch [30/50], Step [1100/1763], Loss: 1.1151\n",
            "Epoch [30/50], Step [1200/1763], Loss: 1.1572\n",
            "Epoch [30/50], Step [1300/1763], Loss: 1.1259\n",
            "Epoch [30/50], Step [1400/1763], Loss: 1.0943\n",
            "Epoch [30/50], Step [1500/1763], Loss: 0.9921\n",
            "Epoch [30/50], Step [1600/1763], Loss: 1.0531\n",
            "Epoch [30/50], Step [1700/1763], Loss: 1.0856\n",
            "Epoch [31/50], Step [100/1763], Loss: 1.0245\n",
            "Epoch [31/50], Step [200/1763], Loss: 1.0558\n",
            "Epoch [31/50], Step [300/1763], Loss: 1.1571\n",
            "Epoch [31/50], Step [400/1763], Loss: 1.1215\n",
            "Epoch [31/50], Step [500/1763], Loss: 1.0344\n",
            "Epoch [31/50], Step [600/1763], Loss: 1.1115\n",
            "Epoch [31/50], Step [700/1763], Loss: 1.0205\n",
            "Epoch [31/50], Step [800/1763], Loss: 1.2315\n",
            "Epoch [31/50], Step [900/1763], Loss: 1.2072\n",
            "Epoch [31/50], Step [1000/1763], Loss: 1.0985\n",
            "Epoch [31/50], Step [1100/1763], Loss: 1.1847\n",
            "Epoch [31/50], Step [1200/1763], Loss: 1.0835\n",
            "Epoch [31/50], Step [1300/1763], Loss: 1.2637\n",
            "Epoch [31/50], Step [1400/1763], Loss: 0.9818\n",
            "Epoch [31/50], Step [1500/1763], Loss: 0.9810\n",
            "Epoch [31/50], Step [1600/1763], Loss: 1.0338\n",
            "Epoch [31/50], Step [1700/1763], Loss: 1.1037\n",
            "Epoch [32/50], Step [100/1763], Loss: 1.0038\n",
            "Epoch [32/50], Step [200/1763], Loss: 1.0087\n",
            "Epoch [32/50], Step [300/1763], Loss: 1.0518\n",
            "Epoch [32/50], Step [400/1763], Loss: 1.1390\n",
            "Epoch [32/50], Step [500/1763], Loss: 1.0488\n",
            "Epoch [32/50], Step [600/1763], Loss: 1.0102\n",
            "Epoch [32/50], Step [700/1763], Loss: 1.1296\n",
            "Epoch [32/50], Step [800/1763], Loss: 1.0904\n",
            "Epoch [32/50], Step [900/1763], Loss: 0.9726\n",
            "Epoch [32/50], Step [1000/1763], Loss: 1.2731\n",
            "Epoch [32/50], Step [1100/1763], Loss: 1.0152\n",
            "Epoch [32/50], Step [1200/1763], Loss: 1.3437\n",
            "Epoch [32/50], Step [1300/1763], Loss: 1.0292\n",
            "Epoch [32/50], Step [1400/1763], Loss: 1.0571\n",
            "Epoch [32/50], Step [1500/1763], Loss: 1.0498\n",
            "Epoch [32/50], Step [1600/1763], Loss: 1.1263\n",
            "Epoch [32/50], Step [1700/1763], Loss: 1.2258\n",
            "Epoch [33/50], Step [100/1763], Loss: 0.9580\n",
            "Epoch [33/50], Step [200/1763], Loss: 1.1467\n",
            "Epoch [33/50], Step [300/1763], Loss: 1.0250\n",
            "Epoch [33/50], Step [400/1763], Loss: 1.0419\n",
            "Epoch [33/50], Step [500/1763], Loss: 1.1619\n",
            "Epoch [33/50], Step [600/1763], Loss: 1.2092\n",
            "Epoch [33/50], Step [700/1763], Loss: 1.0057\n",
            "Epoch [33/50], Step [800/1763], Loss: 0.9927\n",
            "Epoch [33/50], Step [900/1763], Loss: 1.0644\n",
            "Epoch [33/50], Step [1000/1763], Loss: 1.0050\n",
            "Epoch [33/50], Step [1100/1763], Loss: 1.0959\n",
            "Epoch [33/50], Step [1200/1763], Loss: 1.0761\n",
            "Epoch [33/50], Step [1300/1763], Loss: 1.0435\n",
            "Epoch [33/50], Step [1400/1763], Loss: 0.9012\n",
            "Epoch [33/50], Step [1500/1763], Loss: 1.0952\n",
            "Epoch [33/50], Step [1600/1763], Loss: 1.0286\n",
            "Epoch [33/50], Step [1700/1763], Loss: 1.0619\n",
            "Epoch [34/50], Step [100/1763], Loss: 1.0907\n",
            "Epoch [34/50], Step [200/1763], Loss: 1.1719\n",
            "Epoch [34/50], Step [300/1763], Loss: 1.0828\n",
            "Epoch [34/50], Step [400/1763], Loss: 1.0521\n",
            "Epoch [34/50], Step [500/1763], Loss: 0.9927\n",
            "Epoch [34/50], Step [600/1763], Loss: 1.0248\n",
            "Epoch [34/50], Step [700/1763], Loss: 1.1033\n",
            "Epoch [34/50], Step [800/1763], Loss: 1.1145\n",
            "Epoch [34/50], Step [900/1763], Loss: 1.0095\n",
            "Epoch [34/50], Step [1000/1763], Loss: 1.0615\n",
            "Epoch [34/50], Step [1100/1763], Loss: 1.1906\n",
            "Epoch [34/50], Step [1200/1763], Loss: 1.0857\n",
            "Epoch [34/50], Step [1300/1763], Loss: 1.1355\n",
            "Epoch [34/50], Step [1400/1763], Loss: 0.9766\n",
            "Epoch [34/50], Step [1500/1763], Loss: 0.9557\n",
            "Epoch [34/50], Step [1600/1763], Loss: 0.9898\n",
            "Epoch [34/50], Step [1700/1763], Loss: 1.0107\n",
            "Epoch [35/50], Step [100/1763], Loss: 0.9014\n",
            "Epoch [35/50], Step [200/1763], Loss: 1.0437\n",
            "Epoch [35/50], Step [300/1763], Loss: 1.1276\n",
            "Epoch [35/50], Step [400/1763], Loss: 1.0127\n",
            "Epoch [35/50], Step [500/1763], Loss: 1.1250\n",
            "Epoch [35/50], Step [600/1763], Loss: 1.0850\n",
            "Epoch [35/50], Step [700/1763], Loss: 1.1705\n",
            "Epoch [35/50], Step [800/1763], Loss: 1.2049\n",
            "Epoch [35/50], Step [900/1763], Loss: 1.0570\n",
            "Epoch [35/50], Step [1000/1763], Loss: 1.1338\n",
            "Epoch [35/50], Step [1100/1763], Loss: 0.9663\n",
            "Epoch [35/50], Step [1200/1763], Loss: 1.0583\n",
            "Epoch [35/50], Step [1300/1763], Loss: 1.0323\n",
            "Epoch [35/50], Step [1400/1763], Loss: 1.0947\n",
            "Epoch [35/50], Step [1500/1763], Loss: 1.0493\n",
            "Epoch [35/50], Step [1600/1763], Loss: 1.0042\n",
            "Epoch [35/50], Step [1700/1763], Loss: 1.1529\n",
            "Epoch [36/50], Step [100/1763], Loss: 1.0956\n",
            "Epoch [36/50], Step [200/1763], Loss: 1.1620\n",
            "Epoch [36/50], Step [300/1763], Loss: 1.0530\n",
            "Epoch [36/50], Step [400/1763], Loss: 1.1839\n",
            "Epoch [36/50], Step [500/1763], Loss: 1.0703\n",
            "Epoch [36/50], Step [600/1763], Loss: 1.0421\n",
            "Epoch [36/50], Step [700/1763], Loss: 1.0640\n",
            "Epoch [36/50], Step [800/1763], Loss: 1.1219\n",
            "Epoch [36/50], Step [900/1763], Loss: 0.9696\n",
            "Epoch [36/50], Step [1000/1763], Loss: 1.0224\n",
            "Epoch [36/50], Step [1100/1763], Loss: 1.0771\n",
            "Epoch [36/50], Step [1200/1763], Loss: 1.1429\n",
            "Epoch [36/50], Step [1300/1763], Loss: 1.0514\n",
            "Epoch [36/50], Step [1400/1763], Loss: 1.0699\n",
            "Epoch [36/50], Step [1500/1763], Loss: 0.9966\n",
            "Epoch [36/50], Step [1600/1763], Loss: 1.0228\n",
            "Epoch [36/50], Step [1700/1763], Loss: 1.1446\n",
            "Epoch [37/50], Step [100/1763], Loss: 1.0441\n",
            "Epoch [37/50], Step [200/1763], Loss: 1.1265\n",
            "Epoch [37/50], Step [300/1763], Loss: 1.0880\n",
            "Epoch [37/50], Step [400/1763], Loss: 1.0930\n",
            "Epoch [37/50], Step [500/1763], Loss: 1.1051\n",
            "Epoch [37/50], Step [600/1763], Loss: 0.9679\n",
            "Epoch [37/50], Step [700/1763], Loss: 1.1000\n",
            "Epoch [37/50], Step [800/1763], Loss: 1.0337\n",
            "Epoch [37/50], Step [900/1763], Loss: 1.2167\n",
            "Epoch [37/50], Step [1000/1763], Loss: 1.0435\n",
            "Epoch [37/50], Step [1100/1763], Loss: 1.1371\n",
            "Epoch [37/50], Step [1200/1763], Loss: 1.0389\n",
            "Epoch [37/50], Step [1300/1763], Loss: 1.0438\n",
            "Epoch [37/50], Step [1400/1763], Loss: 1.1818\n",
            "Epoch [37/50], Step [1500/1763], Loss: 1.2199\n",
            "Epoch [37/50], Step [1600/1763], Loss: 1.1546\n",
            "Epoch [37/50], Step [1700/1763], Loss: 1.2046\n",
            "Epoch [38/50], Step [100/1763], Loss: 1.0903\n",
            "Epoch [38/50], Step [200/1763], Loss: 1.0361\n",
            "Epoch [38/50], Step [300/1763], Loss: 1.1519\n",
            "Epoch [38/50], Step [400/1763], Loss: 1.2602\n",
            "Epoch [38/50], Step [500/1763], Loss: 1.0055\n",
            "Epoch [38/50], Step [600/1763], Loss: 1.1337\n",
            "Epoch [38/50], Step [700/1763], Loss: 1.0063\n",
            "Epoch [38/50], Step [800/1763], Loss: 1.1195\n",
            "Epoch [38/50], Step [900/1763], Loss: 1.2858\n",
            "Epoch [38/50], Step [1000/1763], Loss: 1.1233\n",
            "Epoch [38/50], Step [1100/1763], Loss: 1.2168\n",
            "Epoch [38/50], Step [1200/1763], Loss: 1.0311\n",
            "Epoch [38/50], Step [1300/1763], Loss: 1.0573\n",
            "Epoch [38/50], Step [1400/1763], Loss: 1.2183\n",
            "Epoch [38/50], Step [1500/1763], Loss: 1.2036\n",
            "Epoch [38/50], Step [1600/1763], Loss: 1.0572\n",
            "Epoch [38/50], Step [1700/1763], Loss: 1.0418\n",
            "Epoch [39/50], Step [100/1763], Loss: 1.1182\n",
            "Epoch [39/50], Step [200/1763], Loss: 1.2189\n",
            "Epoch [39/50], Step [300/1763], Loss: 0.9678\n",
            "Epoch [39/50], Step [400/1763], Loss: 1.2426\n",
            "Epoch [39/50], Step [500/1763], Loss: 1.2462\n",
            "Epoch [39/50], Step [600/1763], Loss: 1.0230\n",
            "Epoch [39/50], Step [700/1763], Loss: 1.1173\n",
            "Epoch [39/50], Step [800/1763], Loss: 1.2774\n",
            "Epoch [39/50], Step [900/1763], Loss: 0.9467\n",
            "Epoch [39/50], Step [1000/1763], Loss: 0.9395\n",
            "Epoch [39/50], Step [1100/1763], Loss: 1.1050\n",
            "Epoch [39/50], Step [1200/1763], Loss: 1.2310\n",
            "Epoch [39/50], Step [1300/1763], Loss: 0.9395\n",
            "Epoch [39/50], Step [1400/1763], Loss: 1.0136\n",
            "Epoch [39/50], Step [1500/1763], Loss: 1.0375\n",
            "Epoch [39/50], Step [1600/1763], Loss: 1.0946\n",
            "Epoch [39/50], Step [1700/1763], Loss: 1.0975\n",
            "Epoch [40/50], Step [100/1763], Loss: 1.1014\n",
            "Epoch [40/50], Step [200/1763], Loss: 1.0648\n",
            "Epoch [40/50], Step [300/1763], Loss: 1.0104\n",
            "Epoch [40/50], Step [400/1763], Loss: 0.9981\n",
            "Epoch [40/50], Step [500/1763], Loss: 1.1111\n",
            "Epoch [40/50], Step [600/1763], Loss: 1.2389\n",
            "Epoch [40/50], Step [700/1763], Loss: 1.0843\n",
            "Epoch [40/50], Step [800/1763], Loss: 1.1617\n",
            "Epoch [40/50], Step [900/1763], Loss: 1.2193\n",
            "Epoch [40/50], Step [1000/1763], Loss: 1.1512\n",
            "Epoch [40/50], Step [1100/1763], Loss: 1.2733\n",
            "Epoch [40/50], Step [1200/1763], Loss: 1.0044\n",
            "Epoch [40/50], Step [1300/1763], Loss: 1.2718\n",
            "Epoch [40/50], Step [1400/1763], Loss: 1.0712\n",
            "Epoch [40/50], Step [1500/1763], Loss: 1.2357\n",
            "Epoch [40/50], Step [1600/1763], Loss: 1.0643\n",
            "Epoch [40/50], Step [1700/1763], Loss: 0.9445\n",
            "Epoch [41/50], Step [100/1763], Loss: 1.0391\n",
            "Epoch [41/50], Step [200/1763], Loss: 1.0803\n",
            "Epoch [41/50], Step [300/1763], Loss: 1.2559\n",
            "Epoch [41/50], Step [400/1763], Loss: 1.0054\n",
            "Epoch [41/50], Step [500/1763], Loss: 1.1532\n",
            "Epoch [41/50], Step [600/1763], Loss: 0.9966\n",
            "Epoch [41/50], Step [700/1763], Loss: 1.0202\n",
            "Epoch [41/50], Step [800/1763], Loss: 1.1666\n",
            "Epoch [41/50], Step [900/1763], Loss: 1.2027\n",
            "Epoch [41/50], Step [1000/1763], Loss: 1.0170\n",
            "Epoch [41/50], Step [1100/1763], Loss: 1.1485\n",
            "Epoch [41/50], Step [1200/1763], Loss: 1.0224\n",
            "Epoch [41/50], Step [1300/1763], Loss: 1.1696\n",
            "Epoch [41/50], Step [1400/1763], Loss: 1.1589\n",
            "Epoch [41/50], Step [1500/1763], Loss: 1.1017\n",
            "Epoch [41/50], Step [1600/1763], Loss: 1.0826\n",
            "Epoch [41/50], Step [1700/1763], Loss: 1.1583\n",
            "Epoch [42/50], Step [100/1763], Loss: 1.0544\n",
            "Epoch [42/50], Step [200/1763], Loss: 1.1283\n",
            "Epoch [42/50], Step [300/1763], Loss: 1.1122\n",
            "Epoch [42/50], Step [400/1763], Loss: 1.1295\n",
            "Epoch [42/50], Step [500/1763], Loss: 1.0638\n",
            "Epoch [42/50], Step [600/1763], Loss: 1.1289\n",
            "Epoch [42/50], Step [700/1763], Loss: 1.1063\n",
            "Epoch [42/50], Step [800/1763], Loss: 1.0150\n",
            "Epoch [42/50], Step [900/1763], Loss: 0.9923\n",
            "Epoch [42/50], Step [1000/1763], Loss: 1.1549\n",
            "Epoch [42/50], Step [1100/1763], Loss: 1.1800\n",
            "Epoch [42/50], Step [1200/1763], Loss: 0.9887\n",
            "Epoch [42/50], Step [1300/1763], Loss: 1.0606\n",
            "Epoch [42/50], Step [1400/1763], Loss: 1.0502\n",
            "Epoch [42/50], Step [1500/1763], Loss: 1.0957\n",
            "Epoch [42/50], Step [1600/1763], Loss: 1.0383\n",
            "Epoch [42/50], Step [1700/1763], Loss: 1.0142\n",
            "Epoch [43/50], Step [100/1763], Loss: 1.0750\n",
            "Epoch [43/50], Step [200/1763], Loss: 1.0456\n",
            "Epoch [43/50], Step [300/1763], Loss: 1.0087\n",
            "Epoch [43/50], Step [400/1763], Loss: 1.0515\n",
            "Epoch [43/50], Step [500/1763], Loss: 1.1683\n",
            "Epoch [43/50], Step [600/1763], Loss: 1.1790\n",
            "Epoch [43/50], Step [700/1763], Loss: 1.2743\n",
            "Epoch [43/50], Step [800/1763], Loss: 1.0776\n",
            "Epoch [43/50], Step [900/1763], Loss: 1.0423\n",
            "Epoch [43/50], Step [1000/1763], Loss: 1.2560\n",
            "Epoch [43/50], Step [1100/1763], Loss: 1.1757\n",
            "Epoch [43/50], Step [1200/1763], Loss: 1.0086\n",
            "Epoch [43/50], Step [1300/1763], Loss: 1.0207\n",
            "Epoch [43/50], Step [1400/1763], Loss: 1.1749\n",
            "Epoch [43/50], Step [1500/1763], Loss: 1.0881\n",
            "Epoch [43/50], Step [1600/1763], Loss: 1.0341\n",
            "Epoch [43/50], Step [1700/1763], Loss: 1.0996\n",
            "Epoch [44/50], Step [100/1763], Loss: 1.0410\n",
            "Epoch [44/50], Step [200/1763], Loss: 1.1229\n",
            "Epoch [44/50], Step [300/1763], Loss: 1.2051\n",
            "Epoch [44/50], Step [400/1763], Loss: 1.2199\n",
            "Epoch [44/50], Step [500/1763], Loss: 1.1232\n",
            "Epoch [44/50], Step [600/1763], Loss: 1.0726\n",
            "Epoch [44/50], Step [700/1763], Loss: 0.9778\n",
            "Epoch [44/50], Step [800/1763], Loss: 1.0418\n",
            "Epoch [44/50], Step [900/1763], Loss: 1.0852\n",
            "Epoch [44/50], Step [1000/1763], Loss: 1.1297\n",
            "Epoch [44/50], Step [1100/1763], Loss: 1.0552\n",
            "Epoch [44/50], Step [1200/1763], Loss: 1.0734\n",
            "Epoch [44/50], Step [1300/1763], Loss: 0.9883\n",
            "Epoch [44/50], Step [1400/1763], Loss: 1.0051\n",
            "Epoch [44/50], Step [1500/1763], Loss: 0.9999\n",
            "Epoch [44/50], Step [1600/1763], Loss: 1.1713\n",
            "Epoch [44/50], Step [1700/1763], Loss: 1.0750\n",
            "Epoch [45/50], Step [100/1763], Loss: 0.9963\n",
            "Epoch [45/50], Step [200/1763], Loss: 1.2315\n",
            "Epoch [45/50], Step [300/1763], Loss: 1.2152\n",
            "Epoch [45/50], Step [400/1763], Loss: 1.0465\n",
            "Epoch [45/50], Step [500/1763], Loss: 1.1724\n",
            "Epoch [45/50], Step [600/1763], Loss: 0.9522\n",
            "Epoch [45/50], Step [700/1763], Loss: 1.2257\n",
            "Epoch [45/50], Step [800/1763], Loss: 1.2407\n",
            "Epoch [45/50], Step [900/1763], Loss: 1.0623\n"
          ]
        }
      ],
      "source": [
        "final_hyperparams = {\n",
        "'Learning Rate Scheduler': 'StepLR',\n",
        "'Activation function': 'relu',\n",
        "'Optimiser': torch.optim.SGD,\n",
        "'Batch Normalisation': True,\n",
        "'Regularisation': (0.0001, 0.0001),\n",
        "'Dropout': False\n",
        "}\n",
        "\n",
        "print(\"\\nTesting:\")\n",
        "print(\"Model Choice MLP\")\n",
        "print(\"{\")\n",
        "for key, value in final_hyperparams.items():\n",
        "    print(f\"{key}: {value}\")   \n",
        "print(\"}\")\n",
        "\n",
        "kfold = KFold(n_splits=5, shuffle=True)\n",
        "accuracy_per_fold = []\n",
        "\n",
        "for fold_idx, (train_idx, val_idx) in enumerate(kfold.split(train_set)):\n",
        "    print(\"Current Fold:\", fold_idx + 1)\n",
        "    train_loader = DataLoader(train_set, batch_size=64, sampler=SubsetRandomSampler(train_idx))\n",
        "    val_loader = DataLoader(train_set, batch_size=64, sampler=SubsetRandomSampler(val_idx))\n",
        "    model = MLP(final_hyperparams).to(device)\n",
        "    NN = Model(model, train_loader.dataset, test_set)\n",
        "    NN.train_loader = train_loader\n",
        "    NN.test_loader = val_loader\n",
        "    NN.train(40, verbose=False) \n",
        "    new_model_accuracy = NN.test() \n",
        "    accuracy_per_fold.append(new_model_accuracy)\n",
        "    model.apply(reset_weights)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
